{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b057db6-3d77-4bf7-bac4-751ffc2cabdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:29:54.145317Z",
     "iopub.status.busy": "2022-11-22T18:29:54.144656Z",
     "iopub.status.idle": "2022-11-22T18:29:54.477372Z",
     "shell.execute_reply": "2022-11-22T18:29:54.475730Z",
     "shell.execute_reply.started": "2022-11-22T18:29:54.145277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dfc0c321a245fa81e0472b7e76bc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, unix_timestamp, to_utc_timestamp, from_unixtime, to_date, dayofweek\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, DateType, LongType, StructField, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcc6292-be11-44f9-8da6-7b0d08f3821c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:30:17.978817Z",
     "iopub.status.busy": "2022-11-22T18:30:17.977886Z",
     "iopub.status.idle": "2022-11-22T18:30:18.242169Z",
     "shell.execute_reply": "2022-11-22T18:30:18.208446Z",
     "shell.execute_reply.started": "2022-11-22T18:30:17.978760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660ff36eab3d4455b04661985b9fb62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "#config.read_file(open('dl.cfg'))\n",
    "\n",
    "#os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27aeb5b-1b2d-41cb-b783-3ae89f94a4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:30:26.303625Z",
     "iopub.status.busy": "2022-11-22T18:30:26.302113Z",
     "iopub.status.idle": "2022-11-22T18:30:26.701986Z",
     "shell.execute_reply": "2022-11-22T18:30:26.700062Z",
     "shell.execute_reply.started": "2022-11-22T18:30:26.303578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48a22cade0f481890bd626accb4fd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    THIS FUNCTION CREATES A SPARK SESSION\n",
    "    INPUT : NONE\n",
    "    OUTPUT : SPARK SESSION\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    \"\"\"spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\"\"\"\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f555de-6182-4af9-a89b-244d76f9dbb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:30:40.989879Z",
     "iopub.status.busy": "2022-11-22T18:30:40.989534Z",
     "iopub.status.idle": "2022-11-22T18:30:41.289919Z",
     "shell.execute_reply": "2022-11-22T18:30:41.288454Z",
     "shell.execute_reply.started": "2022-11-22T18:30:40.989847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b8d75fe17f438499f3776d517d71be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    THIS FUNCTION READS SONG DATA AND CREATES THE SONG AND ARTIST TABLES ALSO SAVES THE OUTPUT TABLE TO AN S3 BUCKET AS A PARQUET FILE\n",
    "    INPUT : \n",
    "    SPARK : WHICH IS A SPARK SESSION\n",
    "    INPUT_DATA : WHICH IS THE PATH OR URL FOR THE SOURCE DATA DIRECTORY \n",
    "    OUTPUT_DATA : TO INDICATES WHERE THE FINAL OUTPUT WILL BE SAVE\"\"\"\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = \"song-data/A/A/A/*.json\"\n",
    "    # simple Note :  i used this path to process only simple amount of data as the are very big and take lots of time\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # if you are running on workspace or locally you have to uncomment this \n",
    "    #song_data = \"song-data/song_data/*/*/*/*.json\"\n",
    "    # read song data file\n",
    "    path = input_data + song_data\n",
    "    \n",
    "    df = spark.read.json(path)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    # first define needed columns\n",
    "    song_table_column_list = [\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\"]\n",
    "    \n",
    "    # selecting the needed columns from the song_data\n",
    "    songs_table = df.select(*song_table_column_list).distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    output_data = \"s3://datalakeprojectegfwddataengineeringudacitymahmoud/songs/\"\n",
    "    songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_id\").parquet(output_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the songs_table\", 50*\"+\")\n",
    "    songs_table.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    \n",
    "    # extract columns to create artists table\n",
    "    # define needed columns list\n",
    "    artist_table_column_list = [\"artist_id\",\"artist_name\",\"artist_location\",\"artist_longitude\",\"artist_latitude\"]\n",
    "    \n",
    "    # selecting the needed columns from the song_data\n",
    "    artists_table = df.select(*artist_table_column_list).distinct()\n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the artists_table\", 50*\"+\")\n",
    "    artists_table.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    \n",
    "    output_data = \"s3://datalakeprojectegfwddataengineeringudacitymahmoud/artists/\"\n",
    "    artists_table.write.mode(\"overwrite\").parquet(output_data)\n",
    "    \n",
    "    #because i need the song table in the process_log_data function so i will\n",
    "    #make a return variable for this function to use it for calling the song_table file\n",
    "    #return songs_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea7140ac-771f-47cf-85e8-b8c9abcbc0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:40:24.524724Z",
     "iopub.status.busy": "2022-11-22T18:40:24.524399Z",
     "iopub.status.idle": "2022-11-22T18:40:24.848749Z",
     "shell.execute_reply": "2022-11-22T18:40:24.847078Z",
     "shell.execute_reply.started": "2022-11-22T18:40:24.524696Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad38229038fb467c9877df65eee92152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    THIS FUNCTION READS LOG DATA AND CREATES THE USER, TIME, SONGPLAY TABLES BY JOINING BETWEEN SONG_DATA AND LOG_DATA \n",
    "    ALSO SAVES THE OUTPUT TABLE TO AN S3 BUCKET AS A PARQUET FILE\n",
    "    \n",
    "    INPUT : \n",
    "    SPARK : WHICH IS A SPARK SESSION\n",
    "    INPUT_DATA : WHICH IS THE PATH OR URL FOR THE SOURCE DATA DIRECTORY \n",
    "    OUTPUT_DATA : TO INDICATES WHERE THE FINAL OUTPUT WILL BE SAVE\"\"\"\n",
    "    \n",
    "    # get filepath to log data file\n",
    "    # in case of cloud use this variable\n",
    "    log_data = \"log-data/2018/11/*.json\"\n",
    "    \n",
    "    \n",
    "    #in case of workspace or local mode use this variable\n",
    "    #log_data = \"log-data/*.json\"\n",
    "    \n",
    "    path = input_data + log_data\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(path)\n",
    "    \n",
    "    def type_adjuster(df):\n",
    "        \"\"\"THIS FUNCTION CHANGES DATA TYPES OF LOG DATA TO ITS PROPER TYPES\n",
    "        INPUT : THE DATA FRAME THAT WE WANT TO CHANGE ITS COULMN\n",
    "        OUTPUT : DATA FRAME WITH ITS TYPES ADJUSTED\"\"\"\n",
    "        df = df.withColumn(\"itemInSession_c\", df[\"itemInSession\"].cast(IntegerType())).drop(\"itemInSession\")\n",
    "        df = df.withColumn(\"sessionId_c\", df[\"sessionId\"].cast(IntegerType())).drop(\"sessionId\")\n",
    "        df = df.withColumn(\"status_c\", df[\"status\"].cast(IntegerType())).drop(\"status\")\n",
    "        df = df.withColumn(\"as_date_C\", to_utc_timestamp(from_unixtime(col(\"ts\")/1000,'yyyy-MM-dd HH:mm:ss'),'EST')).drop(\"ts\")\n",
    "        df = df.withColumn(\"userId_C\", df[\"userId\"].cast(IntegerType())).drop(\"userId\")\n",
    "        return df\n",
    "    \n",
    "    # Calling the type_adjsuter function so that it modifies data types of log_data\n",
    "    df = type_adjuster(df)\n",
    "    \n",
    "    \n",
    "    # Note that i did filter song with page == \"NextSong\" as i am only getting users even if they did not listen to any music\n",
    "    # extract columns for users table    \n",
    "    #specify_needed_columns_list\n",
    "    user_table_column_list = [\"userId_C\",\"firstName\",\"lastName\",\"gender\",\"level\"]\n",
    "    \n",
    "    user_table = df.select(*user_table_column_list).distinct()\n",
    "    \n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the user_table\", 50*\"+\")\n",
    "    user_table.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    output_data = \"s3://datalakeprojectegfwddataengineeringudacitymahmoud/users/\"\n",
    "    user_table.write.mode(\"overwrite\").parquet(output_data)\n",
    "    \n",
    "    # filtering by page name == NextSong to view only the case in which users are listening to music\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    # here i add new columns for each field needed in the time table\n",
    "    time_table = df.withColumn(\"star_time\", date_format(df.as_date_C,\"HH:mm:ss\")).\\\n",
    "    withColumn('hour',date_format(df.as_date_C ,\"h\")).\\\n",
    "    withColumn('day',date_format(df.as_date_C ,\"d\")).\\\n",
    "    withColumn('week',weekofyear(df.as_date_C)).\\\n",
    "    withColumn('month',month(df.as_date_C)).\\\n",
    "    withColumn('year',year(df.as_date_C)).\\\n",
    "    withColumn('week_day',dayofweek(df.as_date_C)).\\\n",
    "    select(\"star_time\",\"hour\",\"day\",\"week\",\"month\",\"year\",\"week_day\").\\\n",
    "    dropDuplicates()\n",
    "    \n",
    "    \"\"\"\n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf()\n",
    "    df = \n",
    "    \"\"\"\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    #time_table = df\n",
    "    \n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the time_table\", 50*\"+\")\n",
    "    time_table.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    output_data = \"s3://datalakeprojectegfwddataengineeringudacitymahmoud/time/\"\n",
    "    time_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data)\n",
    "    \n",
    "    \n",
    "    # here i will start the join process to generate the songplay table\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_data = \"song-data/A/A/A/*.json\"\n",
    "    \n",
    "    path = input_data + song_data\n",
    "    song_df = spark.read.json( path )\n",
    "    \n",
    "    #renaming the df to become log_df\n",
    "    log_df = df\n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the log_df\", 50*\"+\")\n",
    "    log_df.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the song_df\", 50*\"+\")\n",
    "    song_df.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    \n",
    "    # To generate the songplay table we have to join the song_df with the log_df\n",
    "    joined_df = log_df.join(song_df, log_df.artist == song_df.artist_name, 'inner')\n",
    "    \n",
    "    # selecting columns from the joined dataframe\n",
    "    song_play_column_list = [\"songplay_id\",\"as_date_C\",\"userId_C\",\"level\",\"song_id\",\"artist_id\",\"sessionId_c\",\"location\",\"userAgent\"]\n",
    "    \n",
    "    #adding  a column for the songplay id column using a built in function (monotonically_increasing_id)\n",
    "    joined_df = joined_df.withColumn(\"songplay_id\",(monotonically_increasing_id()+1))\n",
    "    \n",
    "    #appying the select statement to pick up only considered columns\n",
    "    joined_df = joined_df.select(*song_play_column_list)\n",
    "    \n",
    "    #extracting the time from the timestamp column (as_data_C) and dropping the old column as_data_C\n",
    "    joined_df = joined_df.withColumn(\"star_time\", date_format(joined_df.as_date_C,\"HH:mm:ss\")).drop(\"as_date_C\")\n",
    "\n",
    "    \n",
    "    # renaming columns\n",
    "    joined_df = joined_df.withColumnRenamed(\"userId_C\",\"user_id\").\\\n",
    "    withColumnRenamed(\"sessionId_c\",\"session_id\")\n",
    "    \n",
    "    # rearranging column positions\n",
    "    song_play_column_list = [\"songplay_id\",\"star_time\",\"user_id\",\"level\",\"song_id\",\"artist_id\",\"session_id\",\"location\",\"userAgent\"]\n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    #just for showing and i will erase it as soon as possible\n",
    "    print(50*\"+\" , \"here is the songplays_table\", 50*\"+\")\n",
    "    joined_df = joined_df.select(*song_play_column_list)\n",
    "    joined_df.limit(1).show(1)\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    # selecting needed columns\n",
    "    #song_df = song_df.select(\"\")\n",
    "\n",
    "    # although it is not needed to add month and year in the songplay but theses two columns are required for partitioning when saving to parquet\n",
    "    songplays_table = joined_df.withColumn(\"year\",year(joined_df.star_time)).withColumn(\"month\", month(joined_df.star_time))\n",
    "    \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    output_data = \"s3://datalakeprojectegfwddataengineeringudacitymahmoud/songplay/\"\n",
    "    songplays_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "391d39d0-55fa-446b-85c5-e4ed5272011c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:42:16.055643Z",
     "iopub.status.busy": "2022-11-22T18:42:16.055321Z",
     "iopub.status.idle": "2022-11-22T18:42:16.392250Z",
     "shell.execute_reply": "2022-11-22T18:42:16.390295Z",
     "shell.execute_reply.started": "2022-11-22T18:42:16.055613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8b29cbc73245fb8d47338c48bf56db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    #in case of cloud use this variable\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"\"\n",
    "    \n",
    "    \n",
    "    # in case of workspace or local mode use this variable\n",
    "    #input_data = \"data/\"\n",
    "    #output_data = \"output/\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "    print(\"Congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e45497-aeb6-4fcf-9ed0-f79782fd5d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:31:17.395817Z",
     "iopub.status.busy": "2022-11-22T18:31:17.395427Z",
     "iopub.status.idle": "2022-11-22T18:31:17.691650Z",
     "shell.execute_reply": "2022-11-22T18:31:17.689957Z",
     "shell.execute_reply.started": "2022-11-22T18:31:17.395783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c414fabdf4d64c64837770a0c488ab1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "895f67ee-a789-4718-9a56-8959d98dedeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T18:42:23.750238Z",
     "iopub.status.busy": "2022-11-22T18:42:23.749605Z",
     "iopub.status.idle": "2022-11-22T18:43:11.354367Z",
     "shell.execute_reply": "2022-11-22T18:43:11.353244Z",
     "shell.execute_reply.started": "2022-11-22T18:42:23.750205Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cde716b57a0471a8ce09095aeff7007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the songs_table ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+------------------+------------+------------------+----+---------+\n",
      "|           song_id|       title|         artist_id|year| duration|\n",
      "+------------------+------------+------------------+----+---------+\n",
      "|SOIGHOD12A8C13B5A1|Indian Angel|ARY589G1187B9A9F4E|2004|171.57179|\n",
      "+------------------+------------+------------------+----+---------+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the artists_table ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+------------------+------------+---------------+----------------+---------------+\n",
      "|         artist_id| artist_name|artist_location|artist_longitude|artist_latitude|\n",
      "+------------------+------------+---------------+----------------+---------------+\n",
      "|ARGE7G11187FB37E05|Cyndi Lauper|   Brooklyn, NY|            null|           null|\n",
      "+------------------+------------+---------------+----------------+---------------+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the user_table ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+--------+---------+--------+------+-----+\n",
      "|userId_C|firstName|lastName|gender|level|\n",
      "+--------+---------+--------+------+-----+\n",
      "|      20|    Aiden| Ramirez|     M| paid|\n",
      "+--------+---------+--------+------+-----+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the time_table ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+---------+----+---+----+-----+----+--------+\n",
      "|star_time|hour|day|week|month|year|week_day|\n",
      "+---------+----+---+----+-----+----+--------+\n",
      "| 08:05:39|   8| 24|  47|   11|2018|       7|\n",
      "+---------+----+---+----+-----+----+--------+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the log_df ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+--------+---------+---------+------+--------+---------+-----+--------------------+------+--------+-----------------+-------------+--------------------+---------------+-----------+--------+-------------------+--------+\n",
      "|  artist|     auth|firstName|gender|lastName|   length|level|            location|method|    page|     registration|         song|           userAgent|itemInSession_c|sessionId_c|status_c|          as_date_C|userId_C|\n",
      "+--------+---------+---------+------+--------+---------+-----+--------------------+------+--------+-----------------+-------------+--------------------+---------------+-----------+--------+-------------------+--------+\n",
      "|Harmonia|Logged In|     Ryan|     M|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|Sehr kosmisch|\"Mozilla/5.0 (X11...|              0|        583|     200|2018-11-15 05:30:26|      26|\n",
      "+--------+---------+---------+------+--------+---------+-----+--------------------+------+--------+-----------------+-------------+--------------------+---------------+-----------+--------+-------------------+--------+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the song_df ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|       title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------+----+\n",
      "|AR10USD1187B99F3F1|           null|Burlington, Ontar...|            null|Tweeterfriendly M...|189.57016|        1|SOHKNRJ12A6701D1F8|Drop of Rain|   0|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+------------+----+\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++ here is the songplays_table ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+-----------+---------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|songplay_id|star_time|user_id|level|           song_id|         artist_id|session_id|            location|           userAgent|\n",
      "+-----------+---------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|25769803777| 22:08:42|     36| paid|SORRNOC12AB017F52B|ARSZ7L31187FB4E610|       957|Janesville-Beloit...|\"Mozilla/5.0 (Win...|\n",
      "+-----------+---------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "\n",
      "Congratulations"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e505f1-d359-46c6-a17e-deec737e8c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
